"""
Core LLM Functions - extracted from ai_backlinking_llm.py
"""
import os
from loguru import logger

# Remove static environment variable loading - will read dynamically
# SERPER_API_KEY = os.getenv("SERPER_API_KEY")
# FIRECRAWL_API_KEY = os.getenv("FIRECRAWL_API_KEY")
# OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
# GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

def _get_openai_api_key():
    """Get OPENAI_API_KEY dynamically from environment"""
    return os.getenv("OPENAI_API_KEY")

def _get_gemini_api_key():
    """Get GEMINI_API_KEY dynamically from environment"""
    return os.getenv("GEMINI_API_KEY")


def llm_text_gen(
    prompt: str,
    provider: str = "gemini",
    model: str | None = None,
    openai_api_key: str | None = None,
    gemini_api_key: str | None = None,
) -> str:
    """Generate text using selected provider (Gemini or OpenAI), with graceful fallback."""
    provider_normalized = (provider or "").strip().lower()
    if provider_normalized in ("gemini", "google", "google-gemini"):
        key = gemini_api_key or _get_gemini_api_key()
        if not key:
            return f"[AI Draft]\n{prompt.strip()}\n\n--\nThis is a placeholder draft (no Gemini API key)."
        try:
            import google.generativeai as genai  # type: ignore
            genai.configure(api_key=key)
            model_name = model or "gemini-2.5-flash"
            gmodel = genai.GenerativeModel(model_name)
            resp = gmodel.generate_content(prompt)
            text = getattr(resp, "text", None)
            return (text or "").strip() or f"[AI Draft]\n{prompt.strip()}\n\n--\nNo content returned."
        except Exception as exc:
            logger.warning(f"Gemini generation failed: {exc}")
            return f"[AI Draft]\n{prompt.strip()}\n\n--\nThis is a placeholder draft (Gemini error)."

    # Default to OpenAI
    key = openai_api_key or _get_openai_api_key()
    if not key:
        return f"[AI Draft]\n{prompt.strip()}\n\n--\nThis is a placeholder draft (no OpenAI API key)."
    try:
        from openai import OpenAI  # type: ignore
        client = OpenAI(api_key=key)
        model_name = model or "gpt-4o-mini"
        response = client.chat.completions.create(
            model=model_name,
            temperature=0.7,
            max_tokens=700,
            messages=[
                {"role": "system", "content": "You are a helpful outreach assistant who writes concise, friendly, highly personalized guest-post outreach emails."},
                {"role": "user", "content": prompt},
            ],
        )
        content = (response.choices[0].message.content or "").strip()
        return content or f"[AI Draft]\n{prompt.strip()}\n\n--\nNo content returned."
    except Exception as exc:
        logger.warning(f"OpenAI generation failed: {exc}")
        return f"[AI Draft]\n{prompt.strip()}\n\n--\nThis is a placeholder draft (OpenAI error)."


def compose_personalized_email(
    website_data,
    insights,
    user_proposal,
    provider: str = "gemini",
    model: str | None = None,
    openai_api_key: str | None = None,
    gemini_api_key: str | None = None,
):
    """
    Compose a personalized outreach email using AI LLM based on website data, insights, and user proposal.

    Args:
        website_data (dict): The data of the website including metadata and contact info.
        insights (str): Insights generated by the LLM about the website.
        user_proposal (dict): The user's proposal for a guest post or content contribution.

    Returns:
        str: A personalized email message.
    """
    # Fix data access - use actual available fields
    contact_name = website_data.get("contact_name", "") or "Webmaster"
    site_name = website_data.get("title", "") or website_data.get("domain", "") or "your site"
    proposed_topic = user_proposal.get("topic", "a guest post")
    user_name = user_proposal.get("user_name", "Your Name")
    user_email = user_proposal.get("user_email", "your_email@example.com")
    
    # Extract additional context for better personalization
    domain = website_data.get("domain", "")
    url = website_data.get("url", "")
    page_excerpt = insights or "No content available"
    
    # Enhanced prompt engineering for Gemini
    email_prompt = f"""
You are an expert outreach specialist who writes highly personalized, compelling guest post proposals. Your goal is to create emails that show genuine research and offer real value to the target website.

TARGET WEBSITE ANALYSIS:
- URL: {url}
- Domain: {domain}
- Site Name: {site_name}
- Content Excerpt: {page_excerpt[:800] if len(page_excerpt) > 800 else page_excerpt}

YOUR PROFILE:
- Name: {user_name}
- Email: {user_email}
- Proposed Topic: {proposed_topic}

CRITICAL REQUIREMENTS:
1. **Personalized Greeting**: Start with "Dear [Domain] Team" or similar professional greeting
2. **Show Research**: Reference 1-2 specific content themes or topics from their site excerpt
3. **Specific Proposal**: Propose a concrete, relevant guest post topic that fits their content strategy
4. **Value Proposition**: Explain clearly what value you'll provide to their audience
5. **Professional Tone**: Keep it friendly but professional, under 150 words
6. **Clear CTA**: Include a specific call-to-action (e.g., "Would you be interested in discussing this opportunity?")
7. **Contact Info**: End with your name and email

EMAIL STRUCTURE:
- Greeting (1 line)
- Research acknowledgment (2-3 lines showing you've studied their content)
- Specific proposal (2-3 lines with concrete topic)
- Value explanation (2-3 lines about benefits)
- Call to action (1-2 lines)
- Professional closing with contact details

IMPORTANT: Make the email feel like it was written specifically for this website based on their actual content. Don't be generic - reference specific themes or topics from their page excerpt.
"""

    return llm_text_gen(
        email_prompt,
        provider=provider,
        model=model,
        openai_api_key=openai_api_key,
        gemini_api_key=gemini_api_key,
    )
